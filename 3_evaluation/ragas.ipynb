{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG\n",
    "\n",
    "The extent to which you can **evaluate** your system is the extent to which you can **improve** your system. Before going to prod, it is in your best interest to establish a framework for quickly and effectively understanding the quality of your RAG application. In this notebook, we will use the RAGAS framework, as proposed by [this paper](https://arxiv.org/pdf/2309.15217), to evaluate the RAG application developed in the previous examples. \n",
    "\n",
    "There is no substitute for reading the paper, but summarized below are the main metrics we will work with. Note: there are many more metrics that can be used depending on use case but these are the main ones covered in the paper so we will start there. \n",
    "\n",
    "# Quality metric breakdown\n",
    "\n",
    "The 3 quality metrics in the RAGAS framework are: **faithfulness**, **answer relevance**, and **context relevance**. Let's take a moment to define each and understand how we can arrive at their values.\n",
    "\n",
    "## Faithfulness\n",
    "\n",
    "An answer to a question can be said to be \"faithful\" if the **claims** that are made in the answer **can be inferred** from the **context**.\n",
    "\n",
    "The process for quantifying this score is as follows:\n",
    "\n",
    "1. Use the following prompt with an LLM to generate shorter more focused statements provided the question and answer.\n",
    "\n",
    "    > Given a question and answer, create one\n",
    "    > or more statements from each sentence\n",
    "    > in the given answer.\n",
    "    > question: [question]\n",
    "    > answer: [answer]\n",
    "\n",
    "2. For each generated statement, verify if it can be inferred from the context with the following prompt.\n",
    "\n",
    "    > Consider the given context and following\n",
    "    > statements, then determine whether they\n",
    "    > are supported by the information present\n",
    "    > in the context. Provide a brief explanation for each statement before arriving\n",
    "    > at the verdict (Yes/No). Provide a final\n",
    "    > verdict for each statement in order at the\n",
    "    > end in the given format. Do not deviate\n",
    "    > from the specified format.\n",
    "    > statement: [statement 1]\n",
    "    > ...\n",
    "    > statement: [statement n]\n",
    "\n",
    "3. The final score can then be calculated Faithfulness = (number of supported statements) / (total number of statements)\n",
    "\n",
    "## Answer Relevance\n",
    "\n",
    "An answer can be said to be relevant if it directly addresses the question (intuitively).\n",
    "\n",
    "The process for quantifying this score is:\n",
    "\n",
    "1. Use an LLM to generate \"hypothetical\" questions to a given answer with the following prompt:\n",
    "\n",
    "    > Generate a question for the given answer.\n",
    "    > answer: [answer]\n",
    "\n",
    "2. Embed the generated \"hypothetical\" questions as vectors.\n",
    "3. Calculate the cosine similarity of the hypothetical questions and the original question, sum those similarities, and divide by n.\n",
    "\n",
    "Expressed computationally: `Answer Relevance = sum(cos_sim((q, q_i) for q_i in n)) / n`\n",
    "\n",
    "## Context Relevance\n",
    "\n",
    "\"The context is considered relevant to the extent that it exclusively contains information that is needed to answer the question.\"\n",
    "\n",
    "The process:\n",
    "\n",
    "1. Use the following LLM prompt to extract a subset of sentences necessary to answer the question. The context is defined as the formatted search result from the vector database.\n",
    "\n",
    "    > Please extract relevant sentences from\n",
    "    > the provided context that can potentially\n",
    "    > help answer the following `{question}`. If no\n",
    "    > relevant sentences are found, or if you\n",
    "    > believe the question cannot be answered\n",
    "    > from the given context, return the phrase\n",
    "    > \"Insufficient Information\". While extracting candidate sentences you’re not allowed to make any changes to sentences\n",
    "    > from given `{context}`.\n",
    "\n",
    "2. Compute the context relevance score = (number of extracted sentences) / (total number of sentences in context)\n",
    "\n",
    "# Let's start coding!\n",
    "\n",
    "If you just finished the other examples this may already be done for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Redis and create chunks to populate the index"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:08:07.766227Z",
     "start_time": "2024-06-12T12:08:07.759763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import warnings\n",
    "import dotenv\n",
    "# mute warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# load env vars from .env file\n",
    "dotenv.load_dotenv()\n",
    "dir_path = os.getcwd()\n",
    "parent_directory = os.path.dirname(dir_path)\n",
    "os.environ[\"ROOT_DIR\"] = parent_directory\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "print(dir_path)\n",
    "print(parent_directory)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rouzbeh.farahmand/PycharmProjects/boa-financial-rag-workshop/3_evaluation\n",
      "/Users/rouzbeh.farahmand/PycharmProjects/boa-financial-rag-workshop\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:09:47.546209Z",
     "start_time": "2024-06-12T12:09:47.263330Z"
    }
   },
   "source": [
    "from redisvl.index import SearchIndex\n",
    "from redisvl.schema import IndexSchema\n",
    "from redis import Redis\n",
    "\n",
    "index_name = 'langchain'\n",
    "prefix = 'chunk'\n",
    "schema = IndexSchema.from_yaml(f'{parent_directory}/helpers/sec_index.yaml')\n",
    "client = Redis.from_url(REDIS_URL)\n",
    "\n",
    "# create an index from schema and the client\n",
    "index = SearchIndex(schema, client)\n",
    "index.create(overwrite=True, drop=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:09:47 redisvl.index.index INFO   Index already exists, overwriting.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure env\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "os.environ[\"ROOT_DIR\"] = parent_directory\n",
    "#setting the local downloaded sentence transformer models folder\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{parent_directory}/models\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:13:14.895145Z",
     "start_time": "2024-06-12T12:12:54.208400Z"
    }
   },
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings \n",
    "from helpers.ingestion import get_sec_data\n",
    "from helpers.ingestion import redis_bulk_upload\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", cache_folder=os.getenv(\"TRANSFORMERS_CACHE\", f\"{parent_directory}/models\"))\n",
    "sec_data = get_sec_data()\n",
    "\n",
    "chunks = redis_bulk_upload(sec_data, index, embeddings, tickers=['AAPL']) #, chunk_size=2500) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✅ Loaded doc info for  110 tickers...\n",
      "✅ Loaded 108 10K chunks for ticker=AAPL from AAPL-2021-10K.pdf\n",
      "✅ Loaded 94 10K chunks for ticker=AAPL from AAPL-2023-10K.pdf\n",
      "✅ Loaded 103 10K chunks for ticker=AAPL from AAPL-2022-10K.pdf\n",
      "✅ Loaded 27 earning_call chunks for ticker=AAPL from 2018-May-01-AAPL.txt\n",
      "✅ Loaded 31 earning_call chunks for ticker=AAPL from 2019-Oct-30-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2016-Jan-26-AAPL.txt\n",
      "✅ Loaded 31 earning_call chunks for ticker=AAPL from 2020-Jul-30-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2017-Aug-01-AAPL.txt\n",
      "✅ Loaded 29 earning_call chunks for ticker=AAPL from 2020-Jan-28-AAPL.txt\n",
      "✅ Loaded 34 earning_call chunks for ticker=AAPL from 2016-Apr-26-AAPL.txt\n",
      "✅ Loaded 29 earning_call chunks for ticker=AAPL from 2017-Jan-31-AAPL.txt\n",
      "✅ Loaded 28 earning_call chunks for ticker=AAPL from 2019-Apr-30-AAPL.txt\n",
      "✅ Loaded 26 earning_call chunks for ticker=AAPL from 2017-Nov-02-AAPL.txt\n",
      "✅ Loaded 31 earning_call chunks for ticker=AAPL from 2016-Oct-25-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2016-Jul-26-AAPL.txt\n",
      "✅ Loaded 27 earning_call chunks for ticker=AAPL from 2017-May-02-AAPL.txt\n",
      "✅ Loaded 32 earning_call chunks for ticker=AAPL from 2019-Jul-30-AAPL.txt\n",
      "✅ Loaded 31 earning_call chunks for ticker=AAPL from 2019-Jan-29-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2018-Jul-31-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2018-Feb-01-AAPL.txt\n",
      "✅ Loaded 33 earning_call chunks for ticker=AAPL from 2018-Nov-01-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2020-Apr-30-AAPL.txt\n",
      "✅✅✅Loaded a total of 874 chunks from 3 10Ks and 19 earning calls for 1 tickers.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:13:18.094811Z",
     "start_time": "2024-06-12T12:13:18.080965Z"
    }
   },
   "source": [
    "flattened_chunks = [item for sublist in chunks for item in sublist]\n",
    "len(flattened_chunks)"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m flattened_chunks \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msublist\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mitem\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msublist\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mlen\u001B[39m(flattened_chunks)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate index and create vector store\n",
    "This is entirely the same as we have done in the previous examples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:14:25.527409Z",
     "start_time": "2024-06-12T12:14:25.511561Z"
    }
   },
   "source": [
    "from langchain_community.vectorstores import Redis as LangChainRedis\n",
    "from helpers.utils import create_langchain_schemas_from_redis_schema\n",
    "\n",
    "index_name = 'langchain'\n",
    "\n",
    "vec_schema , main_schema = create_langchain_schemas_from_redis_schema(f'{parent_directory}/helpers/sec_index.yaml')\n",
    "\n",
    "rds = LangChainRedis.from_existing_index(\n",
    "    embedding=embeddings, \n",
    "    index_name= index_name, \n",
    "    schema = main_schema\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out!\n",
    "We can see the vector store is populated and returning results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:14:30.352851Z",
     "start_time": "2024-06-12T12:14:30.107403Z"
    }
   },
   "source": [
    "rds.similarity_search(\"What was apples revenue last year?\")[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Earlier this month, released macOS Catalina with all new entertainment apps, innovative Sidecar feature that uses iPad to expand Mac workspace and new accessibility tools that enable users to control their Mac entirely with their voice. 1. Catalina brings Apple Arcade experience to Mac. 1. Already seeing some third-party developers bring their iPad apps to Mac App Store with Mac Catalyst, including Twitter, Post-it and more. 4. Launching newly redesigned Mac Pro this fall, which Co. is manufacturing in Austin, Texas. 7. Others: 1. In FY19, crossed $100b in revenue in US for first time. 2. Introduce new services from Apple Card to Apple TV+ and generated over $46b in total Services revenue, setting new yearly Services records in all five geographic segments and driving Services business to size of Fortune 70 co. 3. Delivered new hardware in all device categories. 4. Wearables business showed explosive growth and generated more annual revenue than two-thirds of companies in Fortune 500. 5. Set yearly revenue record for Mac. 6. Outside of iPhone, revenue grew by $17b to almost $118b. 7. Overall success was achieved widely across markets with annual revenue records in US, Canada, Brazil, UK, Germany, France, Italy, Poland, Korea, Malaysia, Philippines and Vietnam. 8. Believes that Co. leads in innovation because AAPL leads with its values. 9.\\n\\nAt time of urgency and action on climate change, continues to drive breakthroughs in clean power, sustainable materials and device recycling. 1. By running 100% of global operations on renewable energy and challenging entire network of suppliers do the same, Co. is driving virtuous cycle of demand for clean sources of power. 2. Sees award Co. recently received from United Nation's Global Climate Action program as mandate to deepen this vital work. 10. Continues to invent and improves on cutting-edge renewable materials, including 100% recycled aluminum alloy found in many of Co.'s products. 1. Added rare earth elements to list of recycled materials with introduction of iPhone 11. 2. Disassembling, recycling or refurbishing millions of devices every year with help of Daisy, recycling robot, and pushing entire global supply chain toward recycled or renewable materials. 11. Driving access to critical coding skills development to educators and students through programs like teaching coding academies and free Everyone Can Code curriculum. 12. Continues to put user privacy at center of everything that Co. does.\", metadata={'id': 'chunk:2019-Oct-30-AAPL.txt-992533b4-acf8-4825-aa49-60ae2cb84048', 'chunk_id': '2019-Oct-30-AAPL.txt-992533b4-acf8-4825-aa49-60ae2cb84048', 'source_doc': '2019-Oct-30-AAPL.txt', 'doc_type': 'earning_call', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# we will use llama3 as our local llm for this use case\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt():\n",
    "    \"\"\"Create the QA chain.\"\"\"\n",
    "    from langchain.prompts import PromptTemplate\n",
    "\n",
    "    # Define our prompt\n",
    "    prompt_template = \"\"\"Use the following pieces of context from financial 10k filings data to answer the user question at the end. Only use the result from tools and evidence provided to you. If you don't know the answer, say that you don't know, don't try to make up an answer. Provide the source of the document that you used to get the answer.\n",
    "\n",
    "    This should be in the following format:\n",
    "\n",
    "    Question: [question here]\n",
    "    Answer: [answer here]\n",
    "    Source: [source document here]\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Context:\n",
    "    ---------\n",
    "    {context}\n",
    "    ---------\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def get_search_kwargs(filters, distance_threshold):\n",
    "    return {\"distance_threshold\":distance_threshold,\"filter\":filters}\n",
    "    \n",
    "\n",
    "# options \n",
    "# search_type=\"similarity_distance_threshold\",\n",
    "# search_kwargs={\"distance_threshold\":0.8, 'include_metadata': True}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=rds.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have our RAG QA to test out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"What was Apple's revenue last year compared to this year??\",\n",
       " 'result': 'Question: What was Apple\\'s revenue last year compared to this year?\\nAnswer: According to the context, in fiscal year \\'18, Apple\\'s revenue grew by $36.4 billion. In addition, Q4 revenue was up 29% over last year, reaching a new September quarter record.\\nSource: The answer is based on the following lines from the text:\\n\"In fiscal year \\'18, Apple\\'s revenue grew by $36.4 billion.\"\\n\"Q4 revenue was up 29% over last year, reaching a new September quarter record.\"',\n",
       " 'source_documents': [Document(page_content=\"Thank you, Nancy. Good afternoon, everyone, and thanks for joining us. I just got back from Brooklyn, where we marked our fourth major launch at the end of the year. In addition to being a great time, it put an exclamation point at the end of a remarkable fiscal 2018. This year, we shipped our 2 billionth iOS device, celebrated the 10th anniversary of the App Store and achieved the strongest revenue and earnings in Apple's history. In fiscal year '18, our revenue grew by $36.4 billion. That's the equivalent of a Fortune 100 company in a single year. And we're capping all that off with our best September quarter ever. Revenue was $62.9 billion, ahead of our expectations. That's an increase of 20% over last year and our highest growth rate in 3 years. We also generated record Q4 earnings with 41% year-over-year growth in EPS. Record results from iPhone, Services and Wearables drove our momentum, and we produced strong double-digit revenue growth in all of our geographic segments. It was a big year and a big quarter for iPhone. Q4 revenue was up 29% over last year, an increase of over $8 billion to a new September quarter record, fueled by continued momentum for iPhone 8, 8 Plus and X and the very successful launch of iPhone Xs and iPhone Xs Max. These latest devices are our most advanced iPhones ever with the industry's first 7-nanometer A12 Bionic chip with an Apple-designed 8-core Neural Engine capable of executing an astounding 5 trillion operations per second.\", metadata={'id': 'chunk:2018-Nov-01-AAPL.txt-7f70c94c-9be0-4055-abf7-cd2970c5ef4a', 'chunk_id': '2018-Nov-01-AAPL.txt-7f70c94c-9be0-4055-abf7-cd2970c5ef4a', 'source_doc': '2018-Nov-01-AAPL.txt', 'doc_type': 'earning_call', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}),\n",
       "  Document(page_content=\"According to App Annie's latest report, App Store continues to build preferred destination for customer purchases by wide margin, generating nearly twice revenue of Google Play. 3. Across all Services offerings, paid subscriptions reached 240m, with growth of 58% over last year. 1. Major contributor to overall strong growth in revenue. 4. Apple Watch: 1. Best qtr. ever. 2. Adding results from Beats and AirPods, total revenue from wearables up almost 70% YoverY. 3. Wearables second largest contributor to revenue growth after iPhone. 1. Started only three years ago. 4. In total, other products category set new all-time record with quarterly revenue exceeding $5b for first time. 5. Mac: 1. Sold 5.1m Macs. 1. Translates to 2% YoverY increase in avg. sales per week. 2. Performance particularly strong in emerging markets with unit sales up 13% YoverY and with all-time records in Latin America, India, Turkey and Central and Eastern Europe. 3. Worldwide, active installed base of Macs up double digits YoverY to new record. 6. iPad: 1. Growth qtr. 2. Sold 13.2m units. 1. Avg. iPad sales per week [up 8%] over last year's Dec. qtr. 3. Sales grew strong double digits in many emerging markets including Latin America, Middle East, Central and Eastern Europe and India, and developed markets including Japan, Australia and Korea. 4. Active installed base of iPad has grown every qtr. since launch in 2010. 1.\", metadata={'id': 'chunk:2018-Feb-01-AAPL.txt-7ce498a4-e708-4f01-a87e-bb63bd022f91', 'chunk_id': '2018-Feb-01-AAPL.txt-7ce498a4-e708-4f01-a87e-bb63bd022f91', 'source_doc': '2018-Feb-01-AAPL.txt', 'doc_type': 'earning_call', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}),\n",
       "  Document(page_content=\"Earlier this month, released macOS Catalina with all new entertainment apps, innovative Sidecar feature that uses iPad to expand Mac workspace and new accessibility tools that enable users to control their Mac entirely with their voice. 1. Catalina brings Apple Arcade experience to Mac. 1. Already seeing some third-party developers bring their iPad apps to Mac App Store with Mac Catalyst, including Twitter, Post-it and more. 4. Launching newly redesigned Mac Pro this fall, which Co. is manufacturing in Austin, Texas. 7. Others: 1. In FY19, crossed $100b in revenue in US for first time. 2. Introduce new services from Apple Card to Apple TV+ and generated over $46b in total Services revenue, setting new yearly Services records in all five geographic segments and driving Services business to size of Fortune 70 co. 3. Delivered new hardware in all device categories. 4. Wearables business showed explosive growth and generated more annual revenue than two-thirds of companies in Fortune 500. 5. Set yearly revenue record for Mac. 6. Outside of iPhone, revenue grew by $17b to almost $118b. 7. Overall success was achieved widely across markets with annual revenue records in US, Canada, Brazil, UK, Germany, France, Italy, Poland, Korea, Malaysia, Philippines and Vietnam. 8. Believes that Co. leads in innovation because AAPL leads with its values. 9.\\n\\nAt time of urgency and action on climate change, continues to drive breakthroughs in clean power, sustainable materials and device recycling. 1. By running 100% of global operations on renewable energy and challenging entire network of suppliers do the same, Co. is driving virtuous cycle of demand for clean sources of power. 2. Sees award Co. recently received from United Nation's Global Climate Action program as mandate to deepen this vital work. 10. Continues to invent and improves on cutting-edge renewable materials, including 100% recycled aluminum alloy found in many of Co.'s products. 1. Added rare earth elements to list of recycled materials with introduction of iPhone 11. 2. Disassembling, recycling or refurbishing millions of devices every year with help of Daisy, recycling robot, and pushing entire global supply chain toward recycled or renewable materials. 11. Driving access to critical coding skills development to educators and students through programs like teaching coding academies and free Everyone Can Code curriculum. 12. Continues to put user privacy at center of everything that Co. does.\", metadata={'id': 'chunk:2019-Oct-30-AAPL.txt-ead3452c-6ef1-4ced-9809-ccd94d42c765', 'chunk_id': '2019-Oct-30-AAPL.txt-ead3452c-6ef1-4ced-9809-ccd94d42c765', 'source_doc': '2019-Oct-30-AAPL.txt', 'doc_type': 'earning_call', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}),\n",
       "  Document(page_content='Revenue almost $6.1b, including $548m received from patent infringement dispute. 1. Excluding that amount, revenue was $5.5b. 1. New all-time record and increase of 15% over last year due in large part to strong growth from apps. 2. Revenue from App Store increased 27%. 1. Number of transacting customers grew 18%; all-time record. 3. Among customers who purchase apps and content from iTunes Stores, avg. amount spent for customer reached an all-time high in Dec. qtr. 8. Other Details: 1. Revenue from other products grew strongly. 1. Up 62% YoverY due to: 1. Growing contribution from Apple Watch. 2. Successful launch of new Apple TV. 2. Both aforementioned established new all-time quarterly records. 2. Expanded Apple Watch distribution significantly over course of qtr. 1. Experienced especially strong results during holiday buying season. 9. Cash Position: 1. 1Q16-end [$215.7b] in cash plus marketable securities. 1. Increased $10.1b sequentially. 2. $200b of this cash or 93% of total was outside US. 2. Returned over $9b to investors. 3. Paid $3b in dividends and equivalents. 4. Spent $3b to repurchase 26m AAPL shares through open market transactions. 5. Launched sixth share repurchase program, spending $3b and receiving an initial delivery of [20.4m shares]. 6. Now completed over $153b of our $200b program, including $110b in share purchases. 7. Plans to provide update on capital return program during 2Q results in April. 1.\\n\\nPlans to be active in US and international debt markets in 2016 in order to fund capital return activities. 2. On 01/26/16, Board of Directors declared cash dividend of $0.52 per share of common stock payable on 02/11/16 to shareholders of record as of 02/08/16. 10. 2Q16 Guidance: 1. Revenue $50-53b. 1. Providing wider range for revenue than usual for 2Q because of volatility seeing in economy and financial and currency markets. 2. GM 39.0-39.5%. 1. Believes these are extremely strong margins in light of headwinds faced from FX and sequential loss of leverage. 3. OpEx $6.0-6.1b. 4. OI&E about $325m. 5. Tax rate about 25.5%. 6. Does not provide guidance beyond current qtr. 1. Difficult to forecast economic and FX factors. 2. At this point, believes March qtr. faces most difficult YoverY compare relative to rest of year.', metadata={'id': 'chunk:2016-Jan-26-AAPL.txt-abe7e2fc-6fbe-4c9e-bd49-03614e4a2088', 'chunk_id': '2016-Jan-26-AAPL.txt-abe7e2fc-6fbe-4c9e-bd49-03614e4a2088', 'source_doc': '2016-Jan-26-AAPL.txt', 'doc_type': 'earning_call', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'})]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What was Apple's revenue last year compared to this year??\"\n",
    "res=qa(query)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup complete!\n",
    "Now let's generate some test questions to evaluate the answering abilities of the RAG QA using the metrics we introduced at the beginning. To do this we can use the LLM to come up with some potential questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is Apple's total revenue for 2023 compared to the previous year?\",\n",
       " 'What percentage increase in Services revenue did Apple report in 2023?',\n",
       " \"How much has Apple's gross margin increased/decreased over the past three years?\",\n",
       " \"What was Apple's operating cash flow for 2023, and how does it compare to 2022?\",\n",
       " 'In what sectors did Apple see significant growth in its hardware sales (e.g., Mac, iPad, etc.)?',\n",
       " \"By what percentage did Apple's iPhone revenue increase or decrease in 2023 compared to the previous year?\",\n",
       " \"What was Apple's research and development expense for 2023, and how does it compare to 2022?\",\n",
       " \"How has Apple's capital expenditures changed over the past five years?\",\n",
       " 'In what regions did Apple see significant growth in its sales (e.g., Asia, Americas, etc.)?',\n",
       " \"By what percentage did Apple's China revenue increase or decrease in 2023 compared to the previous year?\",\n",
       " \"What was Apple's effective tax rate for 2023, and how does it compare to the previous year?\",\n",
       " \"How much has Apple's net income increased/decreased over the past five years?\",\n",
       " \"What is Apple's total debt as of 2023, and how does it compare to 2022?\",\n",
       " 'In what areas did Apple see significant growth in its wearables sales (e.g., Apple Watch, AirPods, etc.)?',\n",
       " \"How much was Apple's dividend per share for 2023, and how does it compare to the previous year?\"]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"evaluation/questions.json\", \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not len(questions):\n",
    "    prompt = \"\"\"\n",
    "        You are a helpful question generating bot.\n",
    "        Generate 15 questions you might ask about Apple's financial performance from it's 2023 annual report, earnings calls,\n",
    "        and other financial documents. Return the response without any additional text as a json object of the form\n",
    "        {\"questions\": [question1, question2, ..., question15]}\n",
    "    \"\"\"\n",
    "\n",
    "    questions = json.loads(llm.generate([prompt]).generations[0][0].text)[\"questions\"]\n",
    "    questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize TestSetGenerator from ragas to generate test questions\n",
    "\n",
    "Note this can be a time consuming process so we have gone ahead and pregenerated this with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q llama-index-embeddings-huggingface llama-index-llms-ollama llama-index-embeddings-instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "generator_llm = Ollama(model=\"llama3\")\n",
    "critic_llm = Ollama(model=\"llama3\")\n",
    "embeddings = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "generator = TestsetGenerator.from_llama_index(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "source = \"../resources/10k/aapl-10k-2023.pdf\"\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[source])\n",
    "\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate testset\n",
    "testset = generator.generate_with_llamaindex_docs(\n",
    "    documents[:30],\n",
    "    test_size=15,\n",
    "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    ")\n",
    "\n",
    "testset.to_pandas().to_csv(\"gen_testset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity here is the question output of the previous steps.\n",
    "testset_questions = [\n",
    "    \"What services does Apple offer through its Payment Services, and how do these services contribute to the company's overall sales?\",\n",
    "    \"What is the estimated maximum one-day loss in fair value of the Company's foreign currency derivative positions, according to the VAR model as of September 24, 2022?\",\n",
    "    \"What drives Apple Inc.'s competitive edge & how does it impact results & financials?\",\n",
    "    \"What are potential risks for Apple if it doesn't meet regulatory expectations or faces antitrust scrutiny, given its ESG investments & reliance on 3rd party data?\",\n",
    "    \"What factors contributed to the 7% boost in iPhone sales and how did this growth align with Apple's recent product launches and industry developments?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_questions = [\n",
    "    \"What services does Apple offer through its Payment Services, and how do these services contribute to the company's overall sales?\",\n",
    "    \"What is the estimated maximum one-day loss in fair value of the Company's foreign currency derivative positions, according to the VAR model as of September 24, 2022?\",\n",
    "    \"What drives Apple Inc.'s competitive edge & how does it impact results & financials?\",\n",
    "    \"What are potential risks for Apple if it doesn't meet regulatory expectations or faces antitrust scrutiny, given its ESG investments & reliance on 3rd party data?\",\n",
    "    \"What factors contributed to the 7% boost in iPhone sales and how did this growth align with Apple's recent product launches and industry developments?\",\n",
    "    \"What were the primary factors driving the increase in Americas' net sales in 2022 compared to 2021?\",\n",
    "    \"What impact do new product and service introductions have on Apple's net sales, cost of sales, and operating expenses?\",\n",
    "    \"What are the main characteristics of the Company's manufacturing purchase obligations as of September 24, 2022?\",\n",
    "    \"What is the trading symbol for Apple Inc.'s common stock?\",\n",
    "    \"What is the estimated maximum one-day loss in fair value of the Company's foreign currency derivative positions as of September 24, 2022?\",\n",
    "    \"What are the potential consequences if Apple Inc. fails to obtain licenses for third-party intellectual property or uses such intellectual property on unreasonable terms?\",\n",
    "    \"What role does the company culture play in its ability to recruit and retain highly skilled employees, and how might it impact the business if not managed effectively?\",\n",
    "    \"What drove iPhone net sales growth in 2022, considering Q4 2021 saw new models released?\",\n",
    "    \"What OS choices support Apple's diverse product lineup?\",\n",
    "    \"What drives changes in Apple's effective tax rates, and how do factors like earnings mix, statutory tax rates, and tax laws influence these fluctuations?\",\n",
    "    \"What could go awry for Apple's top-grossing item, leading to a hit on Q2 earnings?\",\n",
    "    \"What obstacles might affect the Company's DRM & security solution progress, potentially straining ties with tech partners?\",\n",
    "    \"How might Apple's performance be impacted if economic headwinds intensify, driving up competition & eroding consumer trust?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for creating test dataset\n",
    "\n",
    "In the following code we take a list of questions and a QA retrieval chain as input. We call the chain and store the answer returned along with the context (aka source documents) to be used as the essential data for our evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define reusable helper function for evaluating our test set against different chains\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_relevancy,\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "def parse_contexts(source_docs):\n",
    "    return [doc.page_content for doc in source_docs]\n",
    "\n",
    "def create_evaluation_dataset(chain, questions):\n",
    "    res_set = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [],\n",
    "    }\n",
    "\n",
    "    for question in questions:\n",
    "        # call QA chain\n",
    "        result = chain(question)\n",
    "\n",
    "        res_set[\"question\"].append(question)\n",
    "        res_set[\"answer\"].append(result[\"result\"])\n",
    "        res_set[\"contexts\"].append(parse_contexts(result[\"source_documents\"]))\n",
    "    return Dataset.from_dict(res_set)\n",
    "\n",
    "def evaluate_chain(chain, questions, test_name):\n",
    "    eval_dataset = create_evaluation_dataset(chain, questions)\n",
    "\n",
    "    eval_result = evaluate(\n",
    "        eval_dataset,\n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_relevancy\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    eval_df = eval_result.to_pandas()\n",
    "    # store the results of our test for future reference in csv\n",
    "    eval_df.to_csv(f\"{test_name}.csv\")\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n",
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n",
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n",
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n",
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n",
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82930ddd20c54979b4089dd1a2852762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import getpass\n",
    "# from llama_index.llms.ollama import Ollama\n",
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# by default ragas evaluation uses OpenAI\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY\")\n",
    "\n",
    "basic_rag_test = evaluate_chain(qa, testset_questions, \"basic_rag_ragas_testset_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.774471</td>\n",
       "      <td>0.941588</td>\n",
       "      <td>0.026878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.372733</td>\n",
       "      <td>0.235102</td>\n",
       "      <td>0.027302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfulness  answer_relevancy  context_relevancy\n",
       "count     18.000000         18.000000          18.000000\n",
       "mean       0.774471          0.941588           0.026878\n",
       "std        0.372733          0.235102           0.027302\n",
       "min        0.000000          0.000000           0.005495\n",
       "25%        0.589286          0.999986           0.010870\n",
       "50%        1.000000          1.000000           0.014110\n",
       "75%        1.000000          1.000000           0.032585\n",
       "max        1.000000          1.000000           0.100000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_rag_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "We can see from the above results that our basic RAG didn't score particularly well. This is okay because now that we have a baseline for the performance of our RAG, we can begin to try different techniques to improve our results. The reason it is so important to have a framework in place for evaluation is now we can properly experiment with different techniques to see what improves our particular system.\n",
    "\n",
    "One technique we could try is to implement a parent document retriever. A parent document retriever attempts to optimize two competing objectives within RAG - 1) smaller chunks can lead to better embeddings since there is less context to lose the point (so to speak) 2) larger chunks help retain what could be valuable overall context to retrieval. Parent document retrieval allows for the initial query search on smaller chunks for specificity but returns the larger chunks for more complete context. \n",
    "\n",
    "Let's perform an experiment to see if this technique improves our metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader, UnstructuredFileLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.redis import Redis as LangChainRedis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will make a new index for this example defined directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader, TextLoader\n",
    "\n",
    "\n",
    "# load our multi modal docs\n",
    "source_docs = []\n",
    "\n",
    "for doc in sec_data[\"AAPL\"][\"10K_files\"]:\n",
    "    loader = UnstructuredFileLoader(\n",
    "        doc, mode=\"single\", strategy=\"fast\"\n",
    "    )\n",
    "\n",
    "    source_docs.extend(loader.load())\n",
    "\n",
    "for doc in sec_data[\"AAPL\"][\"transcript_files\"]:\n",
    "    loader = TextLoader(doc)\n",
    "\n",
    "    source_docs.extend(loader.load())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "PARENT_CHUNK_SIZE = 5000\n",
    "CHILD_CHUNK_SIZE = 400\n",
    "\n",
    "# This text splitter is used to create the parent documents aka larger chunks\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_CHUNK_SIZE)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHILD_CHUNK_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings for redis vector store\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it is **critical** that our index includes the `doc_id` field otherwise the parent document linking will not happen correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# with langchain we can manually modify the default vector schema configuration\n",
    "vector_schema = {\n",
    "    \"name\": \"chunk_vector\",        # name of the vector field in langchain\n",
    "    \"algorithm\": \"HNSW\",           # could use HNSW instead\n",
    "    \"dims\": 384,                   # set based on the HF model embedding dimension\n",
    "    \"distance_metric\": \"COSINE\",   # could use EUCLIDEAN or IP\n",
    "    \"datatype\": \"FLOAT32\",\n",
    "}\n",
    "\n",
    "# here we can define the entire schema spec for our index in LangChain\n",
    "index_schema = {\n",
    "    \"vector\": [vector_schema],\n",
    "    \"text\": [{\"name\": \"content\"}, {\"name\": \"doc_id\"}],\n",
    "    \"content_vector_key\": \"chunk_vector\" ,   # name of the vector field in langchain\n",
    "}\n",
    "\n",
    "vector_store = LangChainRedis(\n",
    "    REDIS_URL,\n",
    "    \"child_docs\",\n",
    "    embeddings,\n",
    "    index_schema=index_schema\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "parent_doc_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we are adding the source documents and the ParentDocumentRetriever will automatically split them into parent and child documents\n",
    "parent_doc_retriever.add_documents(source_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The Company evaluates the performance of its reportable segments based on net sales and operating income. Net sales for geographic segments are generally based on the location of customers and sales through the Company’s retail stores located in those geographic locations. Operating income for each segment includes net sales to third parties, related cost of sales and operating expenses directly attributable to the segment. Advertising expenses are generally included in the geographic segment in which the expenditures are incurred. Operating income for each segment excludes other income and expense and certain expenses managed outside the reportable segments. Costs excluded from segment operating income include various corporate expenses such as research and development (“R&D”), corporate marketing expenses, certain share-based compensation expenses, income taxes, various nonrecurring charges and other separately managed general and administrative costs. The Company does not include intercompany transfers between segments for management reporting purposes.\\n\\nNote 2 – Revenue\\n\\nNet sales disaggregated by significant products and services for 2022, 2021 and 2020 were as follows (in millions):\\n\\n2022\\n\\n2021\\n\\n(1)\\n\\niPhone (1) Mac iPad Wearables, Home and Accessories Services\\n\\n(1)\\n\\n(3)\\n\\nTotal net sales\\n\\n(4)\\n\\n(1)(2)\\n\\n$\\n\\n$\\n\\n205,489 $ 40,177 29,292 41,241 78,129 394,328 $\\n\\n191,973 $ 35,190 31,862 38,367 68,425 365,817 $\\n\\n(1) Products net sales include amortization of the deferred value of unspecified software upgrade rights, which are bundled in the\\n\\nsales price of the respective product.\\n\\n(2) Wearables, Home and Accessories net sales include sales of AirPods, Apple TV, Apple Watch, Beats products, HomePod mini\\n\\nand accessories.\\n\\n(3) Services net sales include sales from the Company’s advertising, AppleCare, cloud, digital content, payment and other services. Services net sales also include amortization of the deferred value of services bundled in the sales price of certain products.\\n\\n(4)\\n\\nIncludes $7.5 billion of revenue recognized in 2022 that was included in deferred revenue as of September 25, 2021, $6.7 billion of revenue recognized in 2021 that was included in deferred revenue as of September 26, 2020, and $5.0 billion of revenue recognized in 2020 that was included in deferred revenue as of September 28, 2019.\\n\\nThe Company’s proportion of net sales by disaggregated revenue source was generally consistent for each reportable segment in Note 11, “Segment Information and Geographic Data” for 2022, 2021 and 2020, except in Greater China, where iPhone revenue represented a moderately higher proportion of net sales in 2022 and 2021.\\n\\nAs of September 24, 2022 and September 25, 2021, the Company had total deferred revenue of $12.4 billion and $11.9 billion, respectively. As of September 24, 2022, the Company expects 64% of total deferred revenue to be realized in less than a year, 27% within one-to-two years, 7% within two-to-three years and 2% in greater than three years.\\n\\nApple Inc. | 2022 Form 10-K | 37\\n\\n2020\\n\\n137,781 28,622 23,724 30,620 53,768 274,515\\n\\nNote 3 – Financial Instruments\\n\\nCash, Cash Equivalents and Marketable Securities\\n\\nThe following tables show the Company’s cash, cash equivalents and marketable securities by significant investment category as of September 24, 2022 and September 25, 2021 (in millions):\\n\\n2022\\n\\nAdjusted Cost\\n\\nUnrealized Gains\\n\\nUnrealized Losses\\n\\nFair Value\\n\\nCash and Cash Equivalents\\n\\nCurrent Marketable Securities\\n\\nCash\\n\\n$\\n\\n18,546 $\\n\\n— $\\n\\n— $\\n\\n18,546 $\\n\\n18,546 $\\n\\n— $\\n\\nLevel 1\\n\\n(1) :\\n\\nMoney market funds Mutual funds Subtotal (2) :\\n\\nLevel 2\\n\\n2,929 274\\n\\n3,203\\n\\n— —\\n\\n—\\n\\n— (47)\\n\\n(47)\\n\\n2,929 227\\n\\n3,156\\n\\n2,929 —\\n\\n2,929\\n\\n— 227\\n\\n227\\n\\nU.S. Treasury securities U.S. agency securities Non-U.S. government securities Certificates of deposit and time\\n\\n25,134 5,823 16,948\\n\\n— — 2\\n\\n(1,725) (655) (1,201)\\n\\n23,409 5,168 15,749\\n\\n338 — —\\n\\n5,091 240 8,806\\n\\ndeposits\\n\\nCommercial paper Corporate debt securities Municipal securities Mortgage- and asset-backed\\n\\n2,067 718 87,148 921\\n\\n— — 9 —\\n\\n— — (7,707) (35)\\n\\n2,067 718 79,450 886\\n\\n1,805 28 — —\\n\\n262 690 9,023 266\\n\\nsecurities Subtotal\\n\\n22,553\\n\\n161,312\\n\\n—\\n\\n11\\n\\n(2,593)\\n\\n(13,916)\\n\\n19,960\\n\\n147,407\\n\\n—\\n\\n2,171\\n\\n53\\n\\n24,431\\n\\nTotal\\n\\n(3)\\n\\n$ 183,061 $\\n\\n11 $\\n\\n(13,963) $ 169,109 $\\n\\n23,646 $\\n\\n24,658 $\\n\\n2021\\n\\nCash\\n\\n$\\n\\nAdjusted Cost 17,305 $\\n\\nUnrealized Gains\\n\\n— $\\n\\nUnrealized Losses\\n\\n— $\\n\\nFair Value\\n\\n17,305 $\\n\\nCash and Cash Equivalents\\n\\n17,305 $\\n\\nCurrent Marketable Securities\\n\\n— $\\n\\nLevel 1\\n\\n(1) :\\n\\nMoney market funds Mutual funds Subtotal (2) : Equity securities U.S. Treasury securities U.S. agency securities Non-U.S. government securities Certificates of deposit and time\\n\\nLevel 2\\n\\n9,608 175\\n\\n9,783\\n\\n1,527 22,878 8,949 20,201\\n\\n— 11\\n\\n11\\n\\n— 102 2 211\\n\\n— (1)\\n\\n(1)\\n\\n(564) (77) (64) (101)\\n\\n9,608 185\\n\\n9,793\\n\\n963 22,903 8,887 20,311\\n\\n9,608 —\\n\\n9,608\\n\\n— 3,596 1,775 390\\n\\n— 185\\n\\n185\\n\\n963 6,625 1,930 3,091\\n\\ndeposits', metadata={'source': '/Users/robert.shelton/Documents/boa/financial-vss/resources/filings/AAPL/AAPL-2022-10K.pdf'})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test that the retirever works\n",
    "retrieved_docs = parent_doc_retriever.invoke(\"apples's revenue 2023\")\n",
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the same but use our new retriever\n",
    "parent_doc_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=parent_doc_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f096cb20c0ca4a1b8be0ba73dcd5189d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "parent_doc_test = evaluate_chain(parent_doc_qa, testset_questions, \"parent_doc_ragas_testset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.847594</td>\n",
       "      <td>0.665775</td>\n",
       "      <td>0.033181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.297799</td>\n",
       "      <td>0.484437</td>\n",
       "      <td>0.046002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.013699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfulness  answer_relevancy  context_relevancy\n",
       "count     17.000000         18.000000          18.000000\n",
       "mean       0.847594          0.665775           0.033181\n",
       "std        0.297799          0.484437           0.046002\n",
       "min        0.000000          0.000000           0.000000\n",
       "25%        0.909091          0.000000           0.011190\n",
       "50%        1.000000          0.999999           0.013699\n",
       "75%        1.000000          1.000000           0.029096\n",
       "max        1.000000          1.000000           0.166667"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_doc_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "It appears that implementing the parent doc retriever did not meaningfully impact the performance of our RAG application. This is okay because now we know that we need to look into alternative techniques to improve our results. In this series we have provided many different ways to enhance our RAG application this notebook shows how we might measure which ones is the best for our particular use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "As a review, in this notebook we covered:\n",
    "- why it's important to have an evaluation framework\n",
    "- the basic theory of RAGAS\n",
    "- how to interpret and generate faithfulness, answer_relevancy, and context_relevancy\n",
    "- code to evaluate two different RAG chains to monitor how using a parent document retriever might improve our results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

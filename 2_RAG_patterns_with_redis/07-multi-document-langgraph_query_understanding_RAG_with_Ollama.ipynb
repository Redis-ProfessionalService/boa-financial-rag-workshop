{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425fb020-e864-40ce-a31f-8da40c73d14b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <div><img src=\"../assets/redis_logo.svg\" style=\"width: 130px\"> </div>\n",
    "    <div style=\"display: inline-block; text-align: center; margin-bottom: 10px;\">\n",
    "        <span style=\"font-size: 36px;\"><b>Multi-document RAG based on LangGraph with Query Understanding and Redis Retrieval Agents using Ollama</b></span>\n",
    "        <br />\n",
    "    </div>\n",
    "    <br />\n",
    "</div>\n",
    "\n",
    "# \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we saw in notebook 06, a React agentic approach to search and treating a Redis retriever as a tool, and moreover leaving the reasoning to an LLM could lead to some uncertainty. A react agentic approach might be a better approach for planning in an uncertain situation (maybe a robotic application where parameters of the problem are unknown) and not for a search application where the tasks and steps to take are very well-defined. Any search starts with a query by a user. The first task is to understand the query or user's question. The hardest part is to understand the intent of the user which might need some personalized and contextual understanding of the user - which we will not address here. Nevertheless, the first component of a search application is query understanding. The second component is constructing a refined query via query augmentation, translation of user's query into a combination of target query language and mapping the query into a vector space using an embedding model. After query translation and representation in a vector space, it is time to retrieve the relevant documents. Here we showcase the Redis hybrid search capabilities and its integration in langchain. After fetching the most relevant documents, usually reranking happens (and by reranking we don't necessarily mean Semantic reranking - one could apply business logic in this step; for example, Ad injection and/or reordering based on business and user's context such as location or session information). In this notebook we don't use a reranker but employ a very simple document relevancy checker using the same `llama3` LLM. However, in a more realistic scenario, one should use task-specific or fine-tuned LLMs for each of these steps or even use the existing models or pipelines (query classifier, Learning To Rank, etc.). In fact, it might be a requirement that some models need to run faster/cheaper or be more explainable or certified for complaince reasons (such as Logistic Regression, decision trees, MaxEnt etc.) Please plug in any of such models instead of our generic LLM-based component if needed.      ",
   "id": "7c00b2501be3b1a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Graph](query_understanding_graph.png)",
   "id": "f468c64492a20099"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment Setup",
   "id": "c54f1e55f5310c1b"
  },
  {
   "cell_type": "code",
   "id": "e4958a8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:33:53.617822Z",
     "start_time": "2024-06-13T19:33:53.611522Z"
    }
   },
   "source": [
    "import os\n",
    "import warnings\n",
    "import dotenv\n",
    "# mute warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# load env vars from .env file\n",
    "dotenv.load_dotenv()\n",
    "dir_path = os.getcwd()\n",
    "parent_directory = os.path.dirname(dir_path)\n",
    "os.environ[\"ROOT_DIR\"] = parent_directory\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "print(dir_path)\n",
    "print(parent_directory)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rouzbeh.farahmand/PycharmProjects/boa-financial-rag-workshop/2_RAG_patterns_with_redis\n",
      "/Users/rouzbeh.farahmand/PycharmProjects/boa-financial-rag-workshop\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Install Python Dependencies",
   "id": "2c22327a44dd1c4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:33:56.168232Z",
     "start_time": "2024-06-13T19:33:53.700228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "%pip install -r $ROOT_DIR/requirements.txt"
   ],
   "id": "fe27d19af62e7bc5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### SentenceTransformerEmbeddings Models Cache folder\n",
    "We are using `SentenceTransformerEmbeddings` in this demo and here we specify the cache folder. If you already downloaded the models in a local file system, set this folder here, otherwise the library tries to download the models in this folder if not available locally.\n",
    "\n",
    "In particular, this models will be downloaded if not present in the cache folder:\n",
    "\n",
    "models/models--sentence-transformers--all-MiniLM-L6-v2"
   ],
   "id": "9ba8eaf10e92f692"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:33:56.171793Z",
     "start_time": "2024-06-13T19:33:56.169691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#setting the local downloaded sentence transformer models folder\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{parent_directory}/models\""
   ],
   "id": "c405bdf9381db835",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:33:58.627678Z",
     "start_time": "2024-06-13T19:33:56.172467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                           cache_folder=os.getenv(\"TRANSFORMERS_CACHE\", f\"{parent_directory}/models\"))"
   ],
   "id": "11190e3ee0b4ab1d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Build your Redis index \n",
    "Skip this section if you have already built your index in previous notebook.\n"
   ],
   "id": "30bd69c8360baaa0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:33:58.766896Z",
     "start_time": "2024-06-13T19:33:58.629383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from redisvl.index import SearchIndex\n",
    "from redisvl.schema import IndexSchema\n",
    "from redis import Redis\n",
    "index_name = 'langchain'\n",
    "prefix = 'chunk'\n",
    "schema = IndexSchema.from_yaml('sec_index.yaml')\n",
    "client = Redis.from_url(REDIS_URL)\n",
    "# create an index from schema and the client\n",
    "index = SearchIndex(schema, client)\n",
    "index.create(overwrite=True, drop=True)"
   ],
   "id": "78bbc21dd49fb15f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:33:58 redisvl.index.index INFO   Index already exists, overwriting.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:33:59.683672Z",
     "start_time": "2024-06-13T19:33:58.767467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Skip if you have already done populated your index.\n",
    "from ingestion import get_sec_data\n",
    "from ingestion import redis_bulk_upload\n",
    "\n",
    "sec_data = get_sec_data()"
   ],
   "id": "c1ab8cbe7cdea875",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✅ Loaded doc info for  110 tickers...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:34:49.685387Z",
     "start_time": "2024-06-13T19:33:59.684170Z"
    }
   },
   "cell_type": "code",
   "source": "redis_bulk_upload(sec_data, index, embeddings, tickers=['AAPL','AMZN'])",
   "id": "5b87b9c94d496bc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 108 10K chunks for ticker=AAPL from AAPL-2021-10K.pdf\n",
      "✅ Loaded 94 10K chunks for ticker=AAPL from AAPL-2023-10K.pdf\n",
      "✅ Loaded 103 10K chunks for ticker=AAPL from AAPL-2022-10K.pdf\n",
      "✅ Loaded 27 earning_call chunks for ticker=AAPL from 2018-May-01-AAPL.txt\n",
      "✅ Loaded 31 earning_call chunks for ticker=AAPL from 2019-Oct-30-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2016-Jan-26-AAPL.txt\n",
      "✅ Loaded 31 earning_call chunks for ticker=AAPL from 2020-Jul-30-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2017-Aug-01-AAPL.txt\n",
      "✅ Loaded 29 earning_call chunks for ticker=AAPL from 2020-Jan-28-AAPL.txt\n",
      "✅ Loaded 34 earning_call chunks for ticker=AAPL from 2016-Apr-26-AAPL.txt\n",
      "✅ Loaded 29 earning_call chunks for ticker=AAPL from 2017-Jan-31-AAPL.txt\n",
      "✅ Loaded 28 earning_call chunks for ticker=AAPL from 2019-Apr-30-AAPL.txt\n",
      "✅ Loaded 26 earning_call chunks for ticker=AAPL from 2017-Nov-02-AAPL.txt\n",
      "✅ Loaded 31 earning_call chunks for ticker=AAPL from 2016-Oct-25-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2016-Jul-26-AAPL.txt\n",
      "✅ Loaded 27 earning_call chunks for ticker=AAPL from 2017-May-02-AAPL.txt\n",
      "✅ Loaded 32 earning_call chunks for ticker=AAPL from 2019-Jul-30-AAPL.txt\n",
      "✅ Loaded 31 earning_call chunks for ticker=AAPL from 2019-Jan-29-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2018-Jul-31-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2018-Feb-01-AAPL.txt\n",
      "✅ Loaded 33 earning_call chunks for ticker=AAPL from 2018-Nov-01-AAPL.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AAPL from 2020-Apr-30-AAPL.txt\n",
      "✅ Loaded 125 10K chunks for ticker=AMZN from AMZN-2023-10K.pdf\n",
      "✅ Loaded 117 10K chunks for ticker=AMZN from AMZN-2022-10K.pdf\n",
      "✅ Loaded 118 10K chunks for ticker=AMZN from AMZN-2021-10K.pdf\n",
      "✅ Loaded 20 earning_call chunks for ticker=AMZN from 2020-Jan-30-AMZN.txt\n",
      "✅ Loaded 28 earning_call chunks for ticker=AMZN from 2016-Apr-28-AMZN.txt\n",
      "✅ Loaded 19 earning_call chunks for ticker=AMZN from 2019-Oct-24-AMZN.txt\n",
      "✅ Loaded 24 earning_call chunks for ticker=AMZN from 2020-Apr-30-AMZN.txt\n",
      "✅ Loaded 22 earning_call chunks for ticker=AMZN from 2019-Apr-25-AMZN.txt\n",
      "✅ Loaded 17 earning_call chunks for ticker=AMZN from 2017-Jul-27-AMZN.txt\n",
      "✅ Loaded 20 earning_call chunks for ticker=AMZN from 2018-Jul-26-AMZN.txt\n",
      "✅ Loaded 20 earning_call chunks for ticker=AMZN from 2018-Oct-25-AMZN.txt\n",
      "✅ Loaded 30 earning_call chunks for ticker=AMZN from 2016-Jan-28-AMZN.txt\n",
      "✅ Loaded 21 earning_call chunks for ticker=AMZN from 2018-Feb-01-AMZN.txt\n",
      "✅ Loaded 23 earning_call chunks for ticker=AMZN from 2020-Jul-30-AMZN.txt\n",
      "✅ Loaded 21 earning_call chunks for ticker=AMZN from 2019-Jul-25-AMZN.txt\n",
      "✅ Loaded 21 earning_call chunks for ticker=AMZN from 2017-Apr-27-AMZN.txt\n",
      "✅ Loaded 20 earning_call chunks for ticker=AMZN from 2019-Jan-31-AMZN.txt\n",
      "✅ Loaded 19 earning_call chunks for ticker=AMZN from 2018-Apr-26-AMZN.txt\n",
      "✅ Loaded 22 earning_call chunks for ticker=AMZN from 2016-Oct-27-AMZN.txt\n",
      "✅ Loaded 21 earning_call chunks for ticker=AMZN from 2017-Feb-02-AMZN.txt\n",
      "✅ Loaded 26 earning_call chunks for ticker=AMZN from 2016-Jul-28-AMZN.txt\n",
      "✅ Loaded 19 earning_call chunks for ticker=AMZN from 2017-Oct-26-AMZN.txt\n",
      "✅✅✅Loaded a total of 1647 chunks from 6 10Ks and 38 earning calls for 2 tickers.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "c74e4532",
   "metadata": {},
   "source": "## Redis as a Langchain Retriever\n"
  },
  {
   "cell_type": "code",
   "id": "e50c9efe-4abe-42fa-b35a-05eeeede9ec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:34:49.706155Z",
     "start_time": "2024-06-13T19:34:49.685928Z"
    }
   },
   "source": [
    "from langchain_community.vectorstores import Redis as LangChainRedis\n",
    "from utils import create_langchain_schemas_from_redis_schema\n",
    "\n",
    "index_name = 'langchain'\n",
    "\n",
    "vec_schema , main_schema = create_langchain_schemas_from_redis_schema('sec_index.yaml')\n",
    "\n",
    "rds = LangChainRedis.from_existing_index( embedding = embeddings, \n",
    "                                          index_name = index_name, \n",
    "                                          schema = main_schema)\n",
    "redis_retriever = rds.as_retriever()\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test if the Redis index is working and returning relevant document.",
   "id": "fcc76c0d6a83a135"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:34:49.785335Z",
     "start_time": "2024-06-13T19:34:49.706678Z"
    }
   },
   "cell_type": "code",
   "source": "rds.similarity_search(query=\"Apple in 2022\", k=4, distance_threshold=0.8)",
   "id": "39b95a9507cbf6a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Company’s global operations are subject to complex and changing laws and regulations on subjects, including antitrust; privacy, data security and data localization; consumer protection; advertising, sales, billing and e-commerce; financial services and technology; product liability; intellectual property ownership and infringement; digital platforms; internet, telecommunications, and mobile communications; media, television, film and digital content; availability of third-party software applications and services; labor and employment; anticorruption; import, export and trade; foreign exchange controls and cash repatriation restrictions; anti–money laundering; foreign ownership and investment; tax; and environmental, health and safety, including electronic waste, recycling, and climate change.\\n\\nApple Inc. | 2022 Form 10-K | 13', metadata={'id': 'chunk:AAPL-2022-10K.pdf-96b1d14c-dca0-4776-a4f0-ae617bb6906e', 'chunk_id': 'AAPL-2022-10K.pdf-96b1d14c-dca0-4776-a4f0-ae617bb6906e', 'source_doc': 'AAPL-2022-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}),\n",
       " Document(page_content='The following discussion should be read in conjunction with the consolidated financial statements and accompanying notes included in Part II, Item 8 of this Form 10-K. This section of this Form 10-K generally discusses 2022 and 2021 items and year-to-year comparisons between 2022 and 2021. Discussions of 2020 items and year-to-year comparisons between 2021 and 2020 are not included in this Form 10-K, and can be found in “Management’s Discussion and Analysis of Financial Condition and Results of Operations” in Part II, Item 7 of the Company’s Annual Report on Form 10-K for the fiscal year ended September 25, 2021.\\n\\nFiscal Year Highlights\\n\\nFiscal 2022 Highlights\\n\\nTotal net sales increased 8% or $28.5 billion during 2022 compared to 2021, driven primarily by higher net sales of iPhone, Services and Mac. The weakness in foreign currencies relative to the U.S. dollar had an unfavorable year-over-year impact on all Products and Services net sales during 2022.\\n\\nThe Company announces new product, service and software offerings at various times during the year. Significant announcements during fiscal 2022 included the following:\\n\\nFirst Quarter 2022:\\n\\n•\\n\\nUpdated MacBook Pro 14” and MacBook Pro 16”, powered by the Apple M1 Pro or M1 Max chip; and Third generation of AirPods.\\n\\nSecond Quarter 2022:\\n\\n• • •\\n\\nUpdated iPhone SE with 5G technology; All-new Mac Studio, powered by the Apple M1 Max or M1 Ultra chip; All-new Studio Display™; and Updated iPad Air with 5G technology, powered by the Apple M1 chip.\\n\\nThird Quarter 2022:\\n\\n• •\\n\\nUpdated MacBook Air and MacBook Pro 13”, both powered by the Apple M2 chip; iOS 16, macOS Ventura, iPadOS 16 and watchOS 9, updates to the Company’s operating systems; and Apple Pay Later, a buy now, pay later service.\\n\\nFourth Quarter 2022:\\n\\n• •\\n\\niPhone 14, iPhone 14 Plus, iPhone 14 Pro and iPhone 14 Pro Max; Second generation of AirPods Pro; and Apple Watch Series 8, updated Apple Watch SE and all-new Apple Watch Ultra.\\n\\nIn April 2022, the Company announced an increase to its Program authorization from $315 billion to $405 billion and raised its quarterly dividend from $0.22 to $0.23 per share beginning in May 2022. During 2022, the Company repurchased $90.2 billion of its common stock and paid dividends and dividend equivalents of $14.8 billion.\\n\\nCOVID-19', metadata={'id': 'chunk:AAPL-2022-10K.pdf-ab496bc7-9fab-40c2-a6ce-f1d3abadc4ba', 'chunk_id': 'AAPL-2022-10K.pdf-ab496bc7-9fab-40c2-a6ce-f1d3abadc4ba', 'source_doc': 'AAPL-2022-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}),\n",
       " Document(page_content='In May 2023, the Company announced a new share repurchase program of up to $90 billion and raised its quarterly dividend from $0.23 to $0.24 per share beginning in May 2023. During 2023, the Company repurchased $76.6 billion of its common stock and paid dividends and dividend equivalents of $15.0 billion.\\n\\nMacroeconomic Conditions\\n\\nMacroeconomic conditions, including inﬂation, changes in interest rates, and currency ﬂuctuations, have directly and indirectly impacted, and could in the future materially impact, the Company’s results of operations and ﬁnancial condition.\\n\\nApple Inc. | 2023 Form 10-K | 20\\n\\nSegment Operating Performance\\n\\nThe following table shows net sales by reportable segment for 2023, 2022 and 2021 (dollars in millions):\\n\\n2023\\n\\nChange\\n\\n2022\\n\\nChange\\n\\nNet sales by reportable segment:\\n\\nAmericas Europe Greater China Japan Rest of Asia Paciﬁc\\n\\nTotal net sales\\n\\n$\\n\\n$\\n\\n162,560 94,294 72,559 24,257 29,615 383,285\\n\\n(4)% $ (1)% (2)% (7)% 1 % (3)% $\\n\\n169,658 95,118 74,200 25,977 29,375 394,328\\n\\n11 % $ 7 % 9 % (9)% 11 %\\n\\n8 % $\\n\\nAmericas\\n\\nAmericas net sales decreased 4% or $7.1 billion during 2023 compared to 2022 due to lower net sales of iPhone and Mac, partially oﬀset by higher net sales of Services.\\n\\nEurope\\n\\nEurope net sales decreased 1% or $824 million during 2023 compared to 2022. The weakness in foreign currencies relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Europe net sales, which consisted primarily of lower net sales of Mac and Wearables, Home and Accessories, partially oﬀset by higher net sales of iPhone and Services.\\n\\nGreater China\\n\\nGreater China net sales decreased 2% or $1.6 billion during 2023 compared to 2022. The weakness in the renminbi relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Greater China net sales, which consisted primarily of lower net sales of Mac and iPhone.\\n\\nJapan\\n\\nJapan net sales decreased 7% or $1.7 billion during 2023 compared to 2022. The weakness in the yen relative to the U.S. dollar accounted for more than the entire year-over-year decrease in Japan net sales, which consisted primarily of lower net sales of iPhone, Wearables, Home and Accessories and Mac.\\n\\nRest of Asia Paciﬁc', metadata={'id': 'chunk:AAPL-2023-10K.pdf-68afb64a-fc83-4343-9818-a3760594a20a', 'chunk_id': 'AAPL-2023-10K.pdf-68afb64a-fc83-4343-9818-a3760594a20a', 'source_doc': 'AAPL-2023-10K.pdf', 'doc_type': '10K', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'}),\n",
       " Document(page_content=\"Coming this fall, Apple TV+ will be new home for world's most creative story tellers, featuring exclusive: 1. Original shows. 2. Movies. 3. Documentaries. 4. Several Major Product Introductions: 1. iMac: 1. Launched new more powerful iMac with dramatic increases in compute and graphics performance, making it great update for consumers and pros alike. 2. For Mac business overall, faced some processor constraints, leading to 5% revenue decline vs. last year. 1. Believes that Mac revenue would have been up vs. last year without those constraints. 2. Does not believe this challenge will have significant impact on 3Q results. 2. iPad: 1. Returned to growth in Greater China. 1. Generated strong double-digit growth in each of Co.'s other geographic segments. 2. Great iPad results were driven primarily by strong customer response to iPad Pro. 3. Late in qtr., launched all-new iPad Air with ultra-thin design, Apple Pencil support and high-end performance powered by A12 Bionic chip. 4. Introduced new iPad Mini, major upgrade for iPad fans who love ultra-portable design. 1. Like new iPad Air, it delivers power of A12 Bionic and support for Apple Pencil. 3. AirPods: 1. Last month, introduced new AirPods. 1. Second generation of world's most popular wireless headphones. 2. Demand has been incredible. 2. With new Co.-designed H1 chip, new AirPods deliver faster connect times, more talk time and convenience of hands-free Hey Siri. 4.\", metadata={'id': 'chunk:2019-Apr-30-AAPL.txt-a4c48958-51a8-47f8-929f-8f36806def7b', 'chunk_id': '2019-Apr-30-AAPL.txt-a4c48958-51a8-47f8-929f-8f36806def7b', 'source_doc': '2019-Apr-30-AAPL.txt', 'doc_type': 'earning_call', 'ticker': 'AAPL', 'company_name': 'APPLE INC', 'sector': 'Information Technology', 'asset_class': 'Equity', 'location': 'United States', 'exchange': 'NASDAQ', 'currency': 'USD', 'market_value': '559365151.11', 'weight': '5.16', 'notional_value': '559365151.11', 'shares': '4305127', 'price': '129.93'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "225d2277-45b2-4ae8-a7d6-62b07fb4a002",
   "metadata": {},
   "source": "## Create different components of your Graph pipeline"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RAG Chain\n",
    "This component is responsible for genereting the final answer given a context we construct using our retrival process "
   ],
   "id": "85f9d6d70a7d2349"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:34:56.496809Z",
     "start_time": "2024-06-13T19:34:49.785886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate,PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "def get_gen_rag_chain():\n",
    "    # LLM\n",
    "    gen_llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "    gen_local_prompt = PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\")\n",
    "\n",
    "    # Chain\n",
    "    gen_rag_chain = gen_local_prompt | gen_llm | StrOutputParser()\n",
    "    return gen_rag_chain\n",
    "\n",
    "q=\"what is the deferred apple revenue in 2022?\"\n",
    "context = \"\"\"As of September 24, 2022 and September 25, 2021, \n",
    "            the Company had total deferred revenue of $12.4 \n",
    "            billion and $11.9 billion, respectively. As of \n",
    "            September 24, 2022, the Company expects 64% of \n",
    "            total deferred revenue to be realized in less \"\"\"\n",
    "\n",
    "my_gen_rag_chain = get_gen_rag_chain()\n",
    "    # Run Gen LLM\n",
    "response = my_gen_rag_chain.invoke({\"context\": context, \"question\": q})\n",
    "print(response)"
   ],
   "id": "ae6b410735d14731",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I don't know the specific deferred apple revenue for 2022. However, I can tell you that as of September 24, 2022, Apple had total deferred revenue of $12.4 billion.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Query Understanding\n",
    "This component is using our basic `llama3` LLM to analyze user's questions and queries and does several tasks in one-shot:\n",
    "   - determines whether a question is relevant to the domain of finance\n",
    "   - determines if the question can be answered better by looking into `10k`s or `earning_calls` \n",
    "   - determines if the question is generally looking for a numeric answer or an explanation. This could be crucial information in the downstream components where we go to find the right answer or verify our answers. An explanation for example is more textual and it can be measured in terms of cosine similarity;however, a numeric answer can be verified by running a tool that does mathematical calculations that is result of a program, for example in a [PAL chain](https://api.python.langchain.com/en/latest/pal_chain/langchain_experimental.pal_chain.base.PALChain.html#).   "
   ],
   "id": "17fecb700639afe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:37:05.871814Z",
     "start_time": "2024-06-13T19:36:51.557765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "EARNING_CALL_FIELD = 'earning_call'\n",
    "FILING_10K_FIELD = '10K'\n",
    "question_str = \"{question}\" \n",
    "\n",
    "def get_question_analyzer():\n",
    "    question_classifier_llm = ChatOllama(model=\"llama3\", format=\"json\", temperature=0)\n",
    "    question_classifier_prompt = PromptTemplate(\n",
    "        template=f\"\"\"Your task is to analyze and classify a question and formulate a new question related to the topic that you found. You have to determine if the answer for the question can be found in \"{EARNING_CALL_FIELD}\" or \"{FILING_10K_FIELD}\" financial filings. So choose either \"{EARNING_CALL_FIELD}\" or \"{FILING_10K_FIELD}\" as the assigned class. If you are unsure assign `None`. Assign three classes of '{EARNING_CALL_FIELD}' or '{FILING_10K_FIELD}' or 'None' as a JSON with a single key 'question_class'. Also add a new key called 'new_question' and try to rewrite the given question based on the class you detected. If the detected class is 'None' or your could find a relevance of the questions to those '{EARNING_CALL_FIELD}' or '{FILING_10K_FIELD}' return 'None' as the new question.\n",
    "          \n",
    "        \n",
    "        Also add a new field in the JSON called 'question_type'. You have to determine if the answer to user's question is likely to be a number or an explanation. if the answer to the question is likely to be number assign 'numeric' and if the answer is likely to be explanation assign `explain` to the 'question_type'.\n",
    "        \n",
    "       Also add a new field in the JSON called 'question_relevancy'. You have to determine if user's question is relevant to the domain of finance or not. If the question is is not relevant to the domain of finance give the value of 'not_relevant' and if it is relevant to the domain of finance give the value of 'relevant'.    \n",
    "        \n",
    "        Only return a valid JSON objects as your response. If you have new information or notes, add a new field in the JSON called `note` and add your explanation in that `note` field.: \\n\\n {question_str}.  \\n \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    question_analyzer = question_classifier_prompt | question_classifier_llm | JsonOutputParser()\n",
    "    return question_analyzer\n",
    "\n",
    "my_question_analyzer = get_question_analyzer()\n",
    "my_question_analyzer.invoke({\"question\": \"what is the aapl revenue in 2022?\"})"
   ],
   "id": "5762c816db4168eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_class': '10K',\n",
       " 'new_question': \"What are Apple's reported revenues for the year 2022?\",\n",
       " 'question_type': 'numeric',\n",
       " 'question_relevancy': 'relevant'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:37:19.345717Z",
     "start_time": "2024-06-13T19:37:09.210819Z"
    }
   },
   "cell_type": "code",
   "source": "my_question_analyzer.invoke({\"question\": \"what was the mood of Tim Cook in the earning calls of 2022?\"})",
   "id": "b4abf26d92fa5a21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_class': 'earning_call',\n",
       " 'new_question': \"What were the key takeaways from Apple's CEO, Tim Cook, during their 2022 earnings calls?\",\n",
       " 'question_type': 'explain',\n",
       " 'question_relevancy': 'relevant'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:37:31.950895Z",
     "start_time": "2024-06-13T19:37:20.696400Z"
    }
   },
   "cell_type": "code",
   "source": "my_question_analyzer.invoke({\"question\": \"Why colorless green ideas are sleeping furiously?\"})",
   "id": "fbb7f12ecd289a63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_class': 'None',\n",
       " 'new_question': 'None',\n",
       " 'question_type': 'explain',\n",
       " 'question_relevancy': 'not_relevant',\n",
       " 'note': 'This question is not related to finance and does not have a clear answer. It appears to be a philosophical or linguistic puzzle.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Retrieval Grader\n",
    "This is a simple component that determines whether a retrieved document is relevant to user's question or not suing a simple strategy. As we demonstrate below, this is not very effective. Firstly, if the documents are presented using a proper embedding model, then the fact that are semantically relevant to user's query is higher than this simple strategy. Here we simply want to show this component as placeholder. We recommend you to either replace this component with a fine-tuned LLM-based reranker or use a Learning To Rank (LTR) model. "
   ],
   "id": "82e0b1a0bf5809f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Retrieval Grader\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "def get_retrieval_grader():\n",
    "    retrieval_grader_llm = ChatOllama(model='llama3', format=\"json\", temperature=0)\n",
    "\n",
    "    retrieval_grader_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "        Here is the user question: {input} \\n\n",
    "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "        input_variables=[\"input\", \"document\"],\n",
    "    )\n",
    "    \n",
    "    retrieval_grader = retrieval_grader_prompt | retrieval_grader_llm | JsonOutputParser()\n",
    "    return retrieval_grader\n",
    "\n",
    "my_retrieval_grader = get_retrieval_grader()\n",
    "\n",
    "question = \"apple revenue in 2022\"\n",
    "docs = redis_retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[0].page_content\n",
    "print(my_retrieval_grader.invoke({\"input\": question, \"document\": doc_txt}))"
   ],
   "id": "6d636fcc416e5b53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_retrieval_grader.invoke({\"input\": \"apple revenue in 2022\", \"document\": \"\"\"\n",
    "We serve consumers through our online and physical stores and focus on selection, price, and convenience. We design our\n",
    "stores to enable hundreds of millions of unique products to be sold by us and by third parties across dozens of product categories.\n",
    "Customers access our offerings through our websites, mobile apps, Alexa, devices, streaming, and physically visiting our stores. We\n",
    "also manufacture and sell electronic devices, including Kindle, Fire tablet, Fire TV , Echo, Ring, and other devices, and we develop\n",
    "and produce media content. We seek to offer our customers low prices, fast and free delivery, easy-to-use functionality, and timely\n",
    "customer service. In addition, we offer Amazon Prime, a membership program that includes unlimited free shipping on over 100\n",
    "million items, access to unlimited streaming of tens of thousands of movies and TV episodes, including Amazon Original content,\n",
    "and other benefits.\n",
    "\"\"\"})"
   ],
   "id": "13b1ec0b06e274bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_retrieval_grader.invoke({\"input\": \"Amazon's revenue in 2022\", \"document\": \"\"\"\n",
    "We serve consumers through our online and physical stores and focus on selection, price, and convenience. We design our\n",
    "stores to enable hundreds of millions of unique products to be sold by us and by third parties across dozens of product categories.\n",
    "Customers access our offerings through our websites, mobile apps, Alexa, devices, streaming, and physically visiting our stores. We\n",
    "also manufacture and sell electronic devices, including Kindle, Fire tablet, Fire TV , Echo, Ring, and other devices, and we develop\n",
    "and produce media content. We seek to offer our customers low prices, fast and free delivery, easy-to-use functionality, and timely\n",
    "customer service. In addition, we offer Amazon Prime, a membership program that includes unlimited free shipping on over 100\n",
    "million items, access to unlimited streaming of tens of thousands of movies and TV episodes, including Amazon Original content,\n",
    "and other benefits.\n",
    "\"\"\"})"
   ],
   "id": "1434635cf6709c5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see this grader, grades this document as relevant. It seems to be relevant to Aamzon but there is no information around revenue.",
   "id": "829df8ffee98f5bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Target Query Construction and Translation\n",
    "To increase the precision of the retrieval and limit the search space - given the fact you might be dealing with billions of documents - we recommend to take advantage of Redis Hybrid search and automatically construct filters based on your metadata. A Simple vector representation simply does not cut it. To demonstrate that consider these two queries:\n",
    "`what was the performance of amzn in 2021?` and `what was the performance of amzn in 2022`. The cosine similarity of these two queries are:\n",
    " "
   ],
   "id": "1b5e5bf644e35239"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "em1 = np.array(embeddings.embed_query(\"what was the performance of amzn in 2021?\"))\n",
    "em2 = np.array(embeddings.embed_query(\"what was the performance of amzn in 2022?\"))\n",
    "cosine = np.dot(em1,em2)/(norm(em1)*norm(em2))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ],
   "id": "b1c6edecf084ee00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cosine = np.dot(em1,em1)/(norm(em1)*norm(em1))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ],
   "id": "45eac14205620a14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The answer to these questions are wildly different. But the high similarity between two queries will result in documents that might be from different years. However, if you apply a simple date NER and filter the document source year, you will eliminate that problem. This problem also could appear in geospatial search where a longitude and latitude of two points could point to the same location but with different precision(e.g 12.12412414 is textually different from 12.1 but could be the same lat/lon). So again you can translate to a proper geo-filter (which Redis supports: see an example [here](https://redis.io/learn/howtos/solutions/geo/getting-started)).   ",
   "id": "70244a316e5160ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So here we have a built a simple toy NER using spacy, and did a simple translation; again as placeholder for a component that we feel is necessary and we will leave it for you to build your own NER (using your internal metadata and language based on the method of your choice)",
   "id": "7843fb814b811882"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helpers.custom_ners import * \n",
    "def custom_redis_query_translator(q):\n",
    "    filters = get_redis_filters(q)\n",
    "    return filters\n",
    "\n",
    "custom_redis_query_translator(\"what was the performance of amzn in 2021 in nasdaq?\")"
   ],
   "id": "e266c6cdb624601e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "custom_redis_query_translator(\"what was the performance of Apple Inc in 2021?\")",
   "id": "b91bce77c915d3b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Putting it all together:  build your RAG logic inside a Graph\n",
    "Now that we have all the components for our RAG logic we will connect them through a graph."
   ],
   "id": "89c2fc571ea60f6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Annotated, TypedDict, Union, Sequence, List\n",
    "from langchain.agents import create_react_agent\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    filters : str\n",
    "    question_relevancy: str\n",
    "    question_class: str\n",
    "    question_type: str\n",
    "    alternate_question: str\n",
    "    question_note: str\n",
    "    rewrite_num: int\n",
    "    generation: str\n",
    "    documents: List[str]\n"
   ],
   "id": "8f03f9e92f8596f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import Literal\n",
    "from custom_ners import get_redis_filters\n",
    "from utils import *\n",
    "\n",
    "MAX_RETRY_COUNT = 2\n",
    "MAX_TOKEN_LIMIT = 1000 #char\n",
    "TOP_DOC_LIMIT = 2 \n",
    "\n",
    "\n",
    "### Edges\n",
    "def check_retrieval_relevancy(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question or not.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    input = state[\"input\"]\n",
    "    documents = state[\"documents\"]\n",
    "    retries = state[\"rewrite_num\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = my_retrieval_grader.invoke({\"input\": input, \"document\": d})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    \n",
    "    if len(documents) - len(filtered_docs) > 2 :\n",
    "        return \"generate\"\n",
    "    elif retries < MAX_RETRY_COUNT:\n",
    "        return \"rewrite\"\n",
    "    elif retries >= MAX_RETRY_COUNT:\n",
    "        return \"generate\"\n",
    "    \n",
    "### Edges\n",
    "def check_question_relevancy(state) -> Literal[\"generate\", \"retrieve\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the asked question is relevant to our domain and if retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    question_relevancy = state[\"question_relevancy\"]\n",
    "    if question_relevancy == 'not_relevant':\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def query_understanding(state):\n",
    "    \"\"\"\n",
    "    Analyzes the input query for better retrieval.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the result of query analysis\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- QUERY Analysis ---\")\n",
    "    def combine_filters(inferred_filters, doc_type_filter=\"10K\", filter_strategy=\"AND\"):\n",
    "        if inferred_filters is None:\n",
    "            return \"@doc_type:{\"+f\"{doc_type_filter}\"+\"}\"\n",
    "        else:\n",
    "            return \"@doc_type:{\"+f\"{doc_type_filter}\"+\"} \" + filter_strategy +f\" ({inferred_filters})\"\n",
    "            \n",
    "        \n",
    "    question = state[\"input\"]\n",
    "    rewrite_num = state[\"rewrite_num\"]\n",
    "    if rewrite_num is None:\n",
    "        rewrite_num = 1\n",
    "    else:\n",
    "        rewrite_num = int(state[\"rewrite_num\"]) + 1\n",
    "    \n",
    "    q_filters = custom_redis_query_translator(question)\n",
    "    question_analysis = my_question_analyzer.invoke({\"question\": question})\n",
    "    print(f\"---QUERY rewrite---question_analysis={question_analysis}\")\n",
    "    question_class = question_analysis[\"question_class\"]\n",
    "    \n",
    "    if question_class != \"None\":\n",
    "        print(f\"---Question Class: Question is Related to {question_class}---\")\n",
    "        applied_filters = combine_filters(q_filters, doc_type_filter=question_class)\n",
    "    else:\n",
    "        applied_filters = combine_filters(q_filters)\n",
    "        \n",
    "    new_question = question_analysis[\"new_question\"]   \n",
    "    \n",
    "    if new_question != \"None\":\n",
    "        print(f\"---Question Analysis: alternate_question is {new_question}---\")\n",
    "        alternate_question = new_question\n",
    "    else:\n",
    "        alternate_question = None\n",
    "    \n",
    "    question_note = None    \n",
    "    if question_analysis.get('note') is not None:\n",
    "        question_note = question_analysis['note']\n",
    "        \n",
    "        \n",
    "    return {\"filters\": applied_filters, \n",
    "            \"question_class\": question_class, \n",
    "            \"question_type\": question_analysis[\"question_type\"],\n",
    "            \"question_relevancy\": question_analysis[\"question_relevancy\"],\n",
    "            \"alternate_question\": alternate_question,\n",
    "            \"question_note\": question_note,\n",
    "            \"rewrite_num\": rewrite_num\n",
    "            }\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "def redis_retriever(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from Redis.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE FROM REDIS---\")\n",
    "    input = state[\"input\"]\n",
    "    alternate_question = state[\"alternate_question\"]\n",
    "    retries = state[\"rewrite_num\"]\n",
    "    filters = state[\"filters\"]\n",
    "    query = input\n",
    "    print(f\"\\t---RETRIEVE FROM REDIS= query={input} alternate_question={alternate_question}- retries={retries}\")\n",
    "    if alternate_question is not None and retries > 1:\n",
    "        query = alternate_question    \n",
    "     \n",
    "    if filters is None:\n",
    "        documents = rds.similarity_search(query=query, k=4, distance_threshold=0.8)\n",
    "    else:\n",
    "        documents = rds.similarity_search(query=query, k=4, distance_threshold=0.8, filter=filters)\n",
    "        \n",
    "    return {\"documents\": documents}\n",
    "\n",
    "    \n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---GENERATE---\")\n",
    "    final_docs = state[\"documents\"]\n",
    "    final_question = state[\"input\"]\n",
    "    question_relevancy = state[\"question_relevancy\"]\n",
    "    note = None\n",
    "    if state.get('note') is None:\n",
    "        note =state[\"question_note\"]\n",
    "    \n",
    "    final_context = \"\"\n",
    "    if question_relevancy == \"not_relevant\":\n",
    "        final_context = f\"Your question does not seem to be relevant to finance. Please only ask questions that are relevant to financials of companies that are usually reported in 10K or earning calls.\"\n",
    "        if note is not None:\n",
    "            final_context = final_context + f\"\\n\\n{note}\"\n",
    "    elif question_relevancy == \"relevant\" and final_docs is not None and len(final_docs) > 0:\n",
    "        final_context = str(\"\\n\".join(format_docs(final_docs[:TOP_DOC_LIMIT])))[:MAX_TOKEN_LIMIT]\n",
    "       \n",
    "    print(f\"DEBUG:GENERATE === question={final_question}\")\n",
    "    print(f\"DEBUG:GENERATE === context={final_context}\")\n",
    "    \n",
    "    # Run Gen LLM\n",
    "    generated_answer = my_gen_rag_chain.invoke({\"context\": final_context, \"question\": final_question})\n",
    "    \n",
    "    print(f\"RFD-DEBUG:GENERATE=== generation={generated_answer}\")\n",
    "    return {\"messages\": [generated_answer], \"generation\": generated_answer}"
   ],
   "id": "993ba7ced09fdc22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"query_understanding_node\", query_understanding)\n",
    "workflow.add_node(\"redis_retriever_node\", redis_retriever)\n",
    "workflow.add_node(\"generate_node\", generate)\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"redis_retriever_node\",\n",
    "    check_retrieval_relevancy,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"rewrite\": \"query_understanding_node\",\n",
    "        \"generate\": \"generate_node\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"query_understanding_node\",\n",
    "    check_question_relevancy,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"retrieve\": \"redis_retriever_node\",\n",
    "        \"generate\": \"generate_node\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_edge(\"generate_node\", END)\n",
    "\n",
    "workflow.set_entry_point(\"query_understanding_node\")\n",
    "\n",
    "# Compile\n",
    "graphapp = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(graphapp.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    pass\n"
   ],
   "id": "b2acd602638ce906",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"input\":\"What was the deferred revenue of aapl in 2022?\",\n",
    "}\n",
    "for output in graphapp.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ],
   "id": "7823529303b98dfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pprint\n",
    "\n",
    "inputs2 = {\n",
    "    \"input\":\"Why colorless green ideas are furiously sleeping?\"\n",
    "}\n",
    "\n",
    "for output2 in graphapp.stream(inputs2):\n",
    "    for key2, value2 in output2.items():\n",
    "        pprint.pprint(f\"Output from node '{key2}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value2, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ],
   "id": "34f4fb3c09949739",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![furiously_sleeping](chomsky.png)",
   "id": "a3d85443ffb20f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs2 = {\n",
    "    \"input\":\"What was the summary of aapl financial data presented in july 2020 earning calls?\"\n",
    "}\n",
    "\n",
    "for output2 in graphapp.stream(inputs2):\n",
    "    for key2, value2 in output2.items():\n",
    "        pprint.pprint(f\"Output from node '{key2}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value2, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ],
   "id": "aa74afdcc4245394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have certainly indexed this data but failed to retrieve. A date NER and and `source_year` filter translation will fix this!   ",
   "id": "13de001bd05628fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Next Steps\n",
    "Implement\n",
    "- Multiple index routing scenarios where based on the detected `query_class` we run retrieval queries on different Redis indices.\n",
    "- Implement a Caching strategy using powerful and superfast Redis features, including `SemanticCache`.\n",
    "- Replace the `RelevancyGrader` by a proper reranking model and node.   "
   ],
   "id": "4b59f853531650fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "The problem of search is still an old problem, and it is about representation of your information and storing it and getting back what you search for via a retrieval method. In the new `RAG` paradigm, information representation is the vector representation. So the more you can capture the nuances of your data using better and better embedding models, the better the `Retrieval`. However, as demonstrated above, we still need to take advantage of our metadata to increase the accuracy and `Augment` the query and also augment the context that we present to the `Generation` component. And most recently, LLMs do a good job of generating proper answers in the target natural languages. The quality of LLMs and their awareness of the domain language increase the quality of generated response.   "
   ],
   "id": "6c18e5f6c0f3b7fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d0aaded7fca1c11",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
